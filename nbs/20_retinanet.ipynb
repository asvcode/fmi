{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e33da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp retinanet\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d8698",
   "metadata": {},
   "source": [
    "# RetinaNet\n",
    "\n",
    "\n",
    "> The code in this section is borrowed from [this](https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/imports/model.py) notebook and is based on [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d344c6",
   "metadata": {},
   "source": [
    "![pipeline](images/retinanet.PNG)\n",
    "\n",
    "[Image Credit](https://arxiv.org/pdf/1708.02002.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad5b60",
   "metadata": {},
   "source": [
    "There are 4 major components:\n",
    "\n",
    "**1) Bottom-up Pathway** - The backbone network in this example `Resnet34`\n",
    "\n",
    "**2) Top-down pathway and Lateral connections** - The top down pathway upsamples the spatially coarser feature maps from higher pyramid levels, and the lateral connections merge the top-down layers and the bottom-up layers with the same spatial size.\n",
    "\n",
    "**3) Classification subnetwork** - Predicts the probability of an object being present at each spatial location for each anchor box and object class.\n",
    "\n",
    "**4) Regression subnetwork** - Regresses the offset for the bounding boxes from the anchor boxes for each ground-truth object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, LongTensor, FloatTensor\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.models.unet import _get_sz_change_idxs, hook_outputs\n",
    "from fastai.layers import init_default, ConvLayer\n",
    "from fastai.callback.hook import model_sizes\n",
    "from fastai.basics import ifnone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62458b4a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv2d(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias=False, init=nn.init.kaiming_normal_):\n",
    "    \"Create and initialize `nn.Conv2d` layer.\"\n",
    "    if padding is None: padding = ks // 2\n",
    "    return init_default(nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias), init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef48bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNet(nn.Module):\n",
    "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n",
    "        super().__init__()\n",
    "        self.n_classes,self.flatten = n_classes,flatten\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sz_change_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n",
    "        self.encoder = encoder\n",
    "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
    "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
    "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])\n",
    "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
    "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n",
    "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n",
    "        \n",
    "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
    "        \"Helper function to create one of the subnet for regression/classification.\"\n",
    "        layers = [ConvLayer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n",
    "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
    "        layers[-1].bias.data.zero_().add_(final_bias)\n",
    "        layers[-1].weight.data.fill_(0)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _apply_transpose(self, func, p_states, n_classes):\n",
    "        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n",
    "        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n",
    "        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n",
    "        if not self.flatten: \n",
    "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
    "            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n",
    "        else:\n",
    "            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
    "        p_states.append(self.p6top7(p_states[-1]))\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "        for i, smooth in enumerate(self.smoothers[:3]):\n",
    "            p_states[i] = smooth(p_states[i])\n",
    "        \n",
    "        self.sizes = [[p.size(2), p.size(3)] for p in p_states]\n",
    "        return [self._apply_transpose(self.box_regressor, p_states, 4),\n",
    "            self._apply_transpose(self.classifier, p_states, self.n_classes)]\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b76c7",
   "metadata": {},
   "source": [
    "### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b04dc",
   "metadata": {},
   "source": [
    "The paper states that the **Focal Loss** is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training.  Focal Loss is an enhancemnet over Cross-Entropy Loss.  \n",
    "\n",
    "One-stage models can suffer from extreme foreground-background class imbalance problems due to dense sampling of possible object locations (anchor boxes).  In RetinaNet, at each pyramid layer there can be thousands of achor boxes and only a few will be assigned to the ground-truth object whilst the vast majority (easy examples) will be assigned to the background class.\n",
    "\n",
    "Focal Loss reduces the loss contribution from the easy examples and hence increase the importance of correcting missclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f51d6c",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "**Scales of anchor boxes** The default is set to [2^0, 2^⅓, 2^⅔ ] which is what the paper uses.\n",
    "\n",
    "\n",
    "**Aspect Ratios of anchor boxes** The default is set to [0.5, 1, 2] which means the anchor boxes will be of aspect ratios 1:2, 1:1, 2:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def activ_to_bbox(acts, anchors, flatten=True):\n",
    "    \"Extrapolate bounding boxes on anchors from the model activations.\"\n",
    "    if flatten:\n",
    "        acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize\n",
    "        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]\n",
    "        sizes = anchors[...,2:] * torch.exp(acts[...,:2])\n",
    "        return torch.cat([centers, sizes], -1)\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46192ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bbox_to_activ(bboxes, anchors, flatten=True):\n",
    "    \"Return the target of the model on `anchors` for the `bboxes`.\"\n",
    "    if flatten:\n",
    "        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n",
    "        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n",
    "        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_class(idxs, n_classes):\n",
    "    target = idxs.new_zeros(len(idxs), n_classes).float()\n",
    "    mask = idxs != 0\n",
    "    i1s = LongTensor(list(range(len(idxs))))\n",
    "    target[i1s[mask],idxs[mask]-1] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_anchors(sizes, ratios, scales, flatten=True):\n",
    "    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n",
    "    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n",
    "    aspects = torch.tensor(aspects).view(-1,2)\n",
    "    anchors = []\n",
    "    for h,w in sizes:\n",
    "        #4 here to have the anchors overlap.\n",
    "        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n",
    "        base_grid = create_grid((h,w)).unsqueeze(1)\n",
    "        n,a = base_grid.size(0),aspects.size(0)\n",
    "        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n",
    "        anchors.append(ancs.view(h,w,a,4))\n",
    "    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_grid(size):\n",
    "    \"Create a grid of a given `size`.\"\n",
    "    H, W = size\n",
    "    grid = FloatTensor(H, W, 2)\n",
    "    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else torch.tensor([0.])\n",
    "    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n",
    "    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else torch.tensor([0.])\n",
    "    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n",
    "    return grid.view(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cthw2tlbr(boxes):\n",
    "    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n",
    "    top_left = boxes[:,:2] - boxes[:,2:]/2\n",
    "    bot_right = boxes[:,:2] + boxes[:,2:]/2\n",
    "    return torch.cat([top_left, bot_right], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47efcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tlbr2cthw(boxes):\n",
    "    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n",
    "    center = (boxes[:,:2] + boxes[:,2:])/2\n",
    "    sizes = boxes[:,2:] - boxes[:,:2]\n",
    "    return torch.cat([center, sizes], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n",
    "    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n",
    "    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n",
    "    if targets.numel() == 0: return matches\n",
    "    ious = IoU_values(anchors, targets)\n",
    "    vals,idxs = torch.max(ious,1)\n",
    "    matches[vals < bkg_thr] = -1\n",
    "    matches[vals > match_thr] = idxs[vals > match_thr]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d375d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def intersection(anchors, targets):\n",
    "    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n",
    "    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n",
    "    a, t = ancs.size(0), tgts.size(0)\n",
    "    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n",
    "    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n",
    "    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n",
    "    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n",
    "    return sizes[...,0] * sizes[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def IoU_values(anchs, targs):\n",
    "    \"Compute the IoU values of `anchors` by `targets`.\"\n",
    "    inter = intersection(anchs, targs)\n",
    "    anc_sz, tgt_sz = anchs[:,2] * anchs[:,3], targs[:,2] * targs[:,3]\n",
    "    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n",
    "    return inter/(union+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d847b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SigmaL1SmoothLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        reg_diff = torch.abs(targ - pred)\n",
    "        reg_loss = torch.where(torch.le(reg_diff, 1/9), 4.5 * torch.pow(reg_diff, 2), reg_diff - 1/18)\n",
    "        return reg_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNetFocalLoss(nn.Module):\n",
    "    def __init__(self, model, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales=None, ratios=None, reg_loss=F.smooth_l1_loss):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss\n",
    "        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])\n",
    "        self.ratios = ifnone(ratios, [1/2,1,2])\n",
    "        \n",
    "    def decodes(self, x):    return (x[0], x[1].argmax(dim=-1))\n",
    "    \n",
    "    def _change_anchors(self, sizes) -> bool:\n",
    "        if not hasattr(self, 'sizes'): return True\n",
    "        for sz1, sz2 in zip(self.sizes, sizes):\n",
    "            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n",
    "        return False\n",
    "    \n",
    "    def _create_anchors(self, sizes, device:torch.device):\n",
    "        self.sizes = sizes\n",
    "        self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device)\n",
    "    \n",
    "    def _unpad(self, bbox_tgt, clas_tgt):\n",
    "        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))\n",
    "        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx\n",
    "    \n",
    "    def _focal_loss(self, clas_pred, clas_tgt):\n",
    "        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n",
    "        ps = torch.sigmoid(clas_pred.detach())\n",
    "        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps\n",
    "        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)\n",
    "        weights.pow_(self.gamma).mul_(alphas)\n",
    "        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n",
    "        return clas_loss\n",
    "        \n",
    "    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n",
    "        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n",
    "        matches = match_anchors(self.anchors, bbox_tgt)\n",
    "        bbox_mask = matches>=0\n",
    "        if bbox_mask.sum() != 0:\n",
    "            bbox_pred = bbox_pred[bbox_mask]\n",
    "            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n",
    "            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))\n",
    "        else: bb_loss = 0.\n",
    "        matches.add_(1)\n",
    "        clas_tgt = clas_tgt + 1\n",
    "        clas_mask = matches>=0\n",
    "        clas_pred = clas_pred[clas_mask]\n",
    "        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])\n",
    "        clas_tgt = clas_tgt[matches[clas_mask]]\n",
    "        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)\n",
    "    \n",
    "    def forward(self, output, bbox_tgts, clas_tgts):\n",
    "        bbox_preds, clas_preds = output\n",
    "        sizes = self.model.sizes\n",
    "        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)\n",
    "        n_classes = clas_preds.size(2)\n",
    "        return sum([self._one_loss(cp, bp, ct, bt)\n",
    "                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ec1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_explore.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 04_pipeline.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_examine.ipynb.\n",
      "Converted 10_wearable.ipynb.\n",
      "Converted 20_retinanet.ipynb.\n",
      "Converted 90_tutorial.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6f350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
