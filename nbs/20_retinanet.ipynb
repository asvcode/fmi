{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e33da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp retinanet\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d8698",
   "metadata": {},
   "source": [
    "# RetinaNet\n",
    "\n",
    "\n",
    "> Credit for this section is from this [this](https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0](https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/imports/model.py) and [this](https://github.com/fastai/course-v3/blob/master/nbs/dl2/pascal.ipynb) notebook and is based on the [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d344c6",
   "metadata": {},
   "source": [
    "![pipeline](images/retinanet.PNG)\n",
    "\n",
    "[Image Credit](https://arxiv.org/pdf/1708.02002.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad5b60",
   "metadata": {},
   "source": [
    "There are 4 major components:\n",
    "\n",
    "**1) Bottom-up Pathway** - The backbone network in this example `Resnet34`\n",
    "\n",
    "**2) Top-down pathway and Lateral connections** - The top down pathway upsamples the spatially coarser feature maps from higher pyramid levels, and the lateral connections merge the top-down layers and the bottom-up layers with the same spatial size.\n",
    "\n",
    "**3) Classification subnetwork** - Predicts the probability of an object being present at each spatial location for each anchor box and object class.\n",
    "\n",
    "**4) Regression subnetwork** - Regresses the offset for the bounding boxes from the anchor boxes for each ground-truth object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, LongTensor, FloatTensor\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.models.unet import _get_sz_change_idxs, hook_outputs\n",
    "from fastai.layers import init_default, ConvLayer\n",
    "from fastai.callback.hook import model_sizes\n",
    "from fastai.basics import ifnone\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "from cycler import cycler\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.patheffects as patheffects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62458b4a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv2d(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias=False, init=nn.init.kaiming_normal_):\n",
    "    \"Create and initialize `nn.Conv2d` layer.\"\n",
    "    if padding is None: padding = ks // 2\n",
    "    return init_default(nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias), init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNet(nn.Module):\n",
    "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n",
    "        super().__init__()\n",
    "        self.n_classes,self.flatten = n_classes,flatten\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sz_change_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n",
    "        self.encoder = encoder\n",
    "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
    "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
    "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])\n",
    "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
    "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n",
    "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n",
    "        \n",
    "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
    "        \"Helper function to create one of the subnet for regression/classification.\"\n",
    "        layers = [ConvLayer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n",
    "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
    "        layers[-1].bias.data.zero_().add_(final_bias)\n",
    "        layers[-1].weight.data.fill_(0)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _apply_transpose(self, func, p_states, n_classes):\n",
    "        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n",
    "        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n",
    "        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n",
    "        if not self.flatten: \n",
    "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
    "            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n",
    "        else:\n",
    "            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
    "        p_states.append(self.p6top7(p_states[-1]))\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "        for i, smooth in enumerate(self.smoothers[:3]):\n",
    "            p_states[i] = smooth(p_states[i])\n",
    "        \n",
    "        self.sizes = [[p.size(2), p.size(3)] for p in p_states]\n",
    "        return [self._apply_transpose(self.box_regressor, p_states, 4),\n",
    "            self._apply_transpose(self.classifier, p_states, self.n_classes)]\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b76c7",
   "metadata": {},
   "source": [
    "### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b04dc",
   "metadata": {},
   "source": [
    "The paper states that the **Focal Loss** is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training.  Focal Loss is an enhancemnet over Cross-Entropy Loss.  \n",
    "\n",
    "One-stage models can suffer from extreme foreground-background class imbalance problems due to dense sampling of possible object locations (anchor boxes).  In RetinaNet, at each pyramid layer there can be thousands of achor boxes and only a few will be assigned to the ground-truth object whilst the vast majority (easy examples) will be assigned to the background class.\n",
    "\n",
    "Focal Loss reduces the loss contribution from the easy examples and hence increase the importance of correcting missclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f51d6c",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "**Scales of anchor boxes** The default is set to [2^0, 2^⅓, 2^⅔ ] which is what the paper uses.\n",
    "\n",
    "\n",
    "**Aspect Ratios of anchor boxes** The default is set to [0.5, 1, 2] which means the anchor boxes will be of aspect ratios 1:2, 1:1, 2:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2381a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def activ_to_bbox(acts, anchors, flatten=True):\n",
    "    \"Extrapolate bounding boxes on anchors from the model activations.\"\n",
    "    if flatten:\n",
    "        acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize\n",
    "        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]\n",
    "        sizes = anchors[...,2:] * torch.exp(acts[...,:2])\n",
    "        return torch.cat([centers, sizes], -1)\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46192ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bbox_to_activ(bboxes, anchors, flatten=True):\n",
    "    \"Return the target of the model on `anchors` for the `bboxes`.\"\n",
    "    if flatten:\n",
    "        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n",
    "        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n",
    "        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_class(idxs, n_classes):\n",
    "    target = idxs.new_zeros(len(idxs), n_classes).float()\n",
    "    mask = idxs != 0\n",
    "    i1s = LongTensor(list(range(len(idxs))))\n",
    "    target[i1s[mask],idxs[mask]-1] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_anchors(sizes, ratios, scales, flatten=True):\n",
    "    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n",
    "    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n",
    "    aspects = torch.tensor(aspects).view(-1,2)\n",
    "    anchors = []\n",
    "    for h,w in sizes:\n",
    "        #4 here to have the anchors overlap.\n",
    "        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n",
    "        base_grid = create_grid((h,w)).unsqueeze(1)\n",
    "        n,a = base_grid.size(0),aspects.size(0)\n",
    "        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n",
    "        anchors.append(ancs.view(h,w,a,4))\n",
    "    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_grid(size):\n",
    "    \"Create a grid of a given `size`.\"\n",
    "    H, W = size\n",
    "    grid = FloatTensor(H, W, 2)\n",
    "    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else torch.tensor([0.])\n",
    "    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n",
    "    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else torch.tensor([0.])\n",
    "    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n",
    "    return grid.view(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cthw2tlbr(boxes):\n",
    "    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n",
    "    top_left = boxes[:,:2] - boxes[:,2:]/2\n",
    "    bot_right = boxes[:,:2] + boxes[:,2:]/2\n",
    "    return torch.cat([top_left, bot_right], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47efcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tlbr2cthw(boxes):\n",
    "    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n",
    "    center = (boxes[:,:2] + boxes[:,2:])/2\n",
    "    sizes = boxes[:,2:] - boxes[:,:2]\n",
    "    return torch.cat([center, sizes], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n",
    "    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n",
    "    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n",
    "    if targets.numel() == 0: return matches\n",
    "    ious = IoU_values(anchors, targets)\n",
    "    vals,idxs = torch.max(ious,1)\n",
    "    matches[vals < bkg_thr] = -1\n",
    "    matches[vals > match_thr] = idxs[vals > match_thr]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d375d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def intersection(anchors, targets):\n",
    "    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n",
    "    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n",
    "    a, t = ancs.size(0), tgts.size(0)\n",
    "    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n",
    "    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n",
    "    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n",
    "    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n",
    "    return sizes[...,0] * sizes[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def IoU_values(anchs, targs):\n",
    "    \"Compute the IoU values of `anchors` by `targets`.\"\n",
    "    inter = intersection(anchs, targs)\n",
    "    anc_sz, tgt_sz = anchs[:,2] * anchs[:,3], targs[:,2] * targs[:,3]\n",
    "    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n",
    "    return inter/(union+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d847b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SigmaL1SmoothLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        reg_diff = torch.abs(targ - pred)\n",
    "        reg_loss = torch.where(torch.le(reg_diff, 1/9), 4.5 * torch.pow(reg_diff, 2), reg_diff - 1/18)\n",
    "        return reg_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008edb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNetFocalLoss(nn.Module):\n",
    "    def __init__(self, model, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales=None, ratios=None, reg_loss=F.smooth_l1_loss):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss\n",
    "        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])\n",
    "        self.ratios = ifnone(ratios, [1/2,1,2])\n",
    "        \n",
    "    def decodes(self, x):    return (x[0], x[1].argmax(dim=-1))\n",
    "    \n",
    "    def _change_anchors(self, sizes) -> bool:\n",
    "        if not hasattr(self, 'sizes'): return True\n",
    "        for sz1, sz2 in zip(self.sizes, sizes):\n",
    "            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n",
    "        return False\n",
    "    \n",
    "    def _create_anchors(self, sizes, device:torch.device):\n",
    "        self.sizes = sizes\n",
    "        self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device)\n",
    "    \n",
    "    def _unpad(self, bbox_tgt, clas_tgt):\n",
    "        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))\n",
    "        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx\n",
    "    \n",
    "    def _focal_loss(self, clas_pred, clas_tgt):\n",
    "        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n",
    "        ps = torch.sigmoid(clas_pred.detach())\n",
    "        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps\n",
    "        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)\n",
    "        weights.pow_(self.gamma).mul_(alphas)\n",
    "        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n",
    "        return clas_loss\n",
    "        \n",
    "    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n",
    "        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n",
    "        matches = match_anchors(self.anchors, bbox_tgt)\n",
    "        bbox_mask = matches>=0\n",
    "        if bbox_mask.sum() != 0:\n",
    "            bbox_pred = bbox_pred[bbox_mask]\n",
    "            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n",
    "            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))\n",
    "        else: bb_loss = 0.\n",
    "        matches.add_(1)\n",
    "        clas_tgt = clas_tgt + 1\n",
    "        clas_mask = matches>=0\n",
    "        clas_pred = clas_pred[clas_mask]\n",
    "        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])\n",
    "        clas_tgt = clas_tgt[matches[clas_mask]]\n",
    "        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)\n",
    "    \n",
    "    def forward(self, output, bbox_tgts, clas_tgts):\n",
    "        bbox_preds, clas_preds = output\n",
    "        sizes = self.model.sizes\n",
    "        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)\n",
    "        n_classes = clas_preds.size(2)\n",
    "        return sum([self._one_loss(cp, bp, ct, bt)\n",
    "                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3003f",
   "metadata": {},
   "source": [
    "### mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541d985",
   "metadata": {},
   "source": [
    ">The code for this section is credited to the following [https://github.com/jaidmin/Practical-Deep-Learning-for-Coders-2.0](https://github.com/jaidmin/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Object_Detection_changed.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54705b",
   "metadata": {},
   "source": [
    "The general definition for **Average Precision(AP)** is finding the area under the Precision-Recall curve. Precision-Recall curves are generally used when there is a moderate to large class imbalance, as opposed to ROC curves which are used when there are roughly equal numbers of observations for each class. \n",
    "\n",
    "To calculate the AP for object detection we need to understand **Intersection of Union(IoU)** which is the ratio of the area of intersection and the area of union (union between predicted bounding box and ground truth bounding box).\n",
    "\n",
    "\n",
    "![pipeline](images/iou.PNG)\n",
    "\n",
    "[Image Credit](https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2)\n",
    "\n",
    "**mAP(mean average precision)** is the average of AP.\n",
    "\n",
    "The mean Average Precision or mAP score is calculated by taking the mean AP over all classes and/or overall IoU thresholds, depending on different detection challenges that exist.\n",
    "\n",
    "For example in the PASCAL VOC2007 challenge, AP for one object class is calculated for an IoU threshold of 0.5. So the mAP is averaged over all object classes and for the COCO 2017 challenge, the mAP is averaged over all object categories and 10 IoU thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727118be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LookUpMetric(Metric):\n",
    "    def __init__(self, reference_metric, metric_name, lookup_idx):\n",
    "        store_attr(self, \"reference_metric,metric_name,lookup_idx\")\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def accumulate(self, learn):\n",
    "        pass\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.reference_metric.res is None:\n",
    "            _ = self.reference_metric.value\n",
    "        return self.reference_metric.res[self.lookup_idx][\"AP\"]\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric_name + \"AP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MethodAveragePrecision(Enum):\n",
    "    \"\"\"\n",
    "    Class representing if the coordinates are relative to the\n",
    "    image size or are absolute values.\n",
    "        Developed by: Rafael Padilla\n",
    "        Last modification: Apr 28 2018\n",
    "    \"\"\"\n",
    "    EveryPointInterpolation = 1\n",
    "    ElevenPointInterpolation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8147f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CoordinatesType(Enum):\n",
    "    \"\"\"\n",
    "    Class representing if the coordinates are relative to the\n",
    "    image size or are absolute values.\n",
    "        Developed by: Rafael Padilla\n",
    "        Last modification: Apr 28 2018\n",
    "    \"\"\"\n",
    "    Relative = 1\n",
    "    Absolute = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf591be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BBType(Enum):\n",
    "    \"\"\"\n",
    "    Class representing if the bounding box is groundtruth or not.\n",
    "        Developed by: Rafael Padilla\n",
    "        Last modification: May 24 2018\n",
    "    \"\"\"\n",
    "    GroundTruth = 1\n",
    "    Detected = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BBFormat(Enum):\n",
    "    \"\"\"\n",
    "    Class representing the format of a bounding box.\n",
    "    It can be (X,Y,width,height) => XYWH\n",
    "    or (X1,Y1,X2,Y2) => XYX2Y2\n",
    "        Developed by: Rafael Padilla\n",
    "        Last modification: May 24 2018\n",
    "    \"\"\"\n",
    "    XYWH = 1\n",
    "    XYX2Y2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convertToRelativeValues(size, box):\n",
    "    dw = 1. / (size[0])\n",
    "    dh = 1. / (size[1])\n",
    "    cx = (box[1] + box[0]) / 2.0\n",
    "    cy = (box[3] + box[2]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = cx * dw\n",
    "    y = cy * dh\n",
    "    w = w * dw\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convertToAbsoluteValues(size, box):\n",
    "    xIn = round(((2 * float(box[0]) - float(box[2])) * size[0] / 2))\n",
    "    yIn = round(((2 * float(box[1]) - float(box[3])) * size[1] / 2))\n",
    "    xEnd = xIn + round(float(box[2]) * size[0])\n",
    "    yEnd = yIn + round(float(box[3]) * size[1])\n",
    "    if xIn < 0:\n",
    "        xIn = 0\n",
    "    if yIn < 0:\n",
    "        yIn = 0\n",
    "    if xEnd >= size[0]:\n",
    "        xEnd = size[0] - 1\n",
    "    if yEnd >= size[1]:\n",
    "        yEnd = size[1] - 1\n",
    "    return (xIn, yIn, xEnd, yEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd11658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_bb_into_image(image, bb, color=(255, 0, 0), thickness=2, label=None):\n",
    "    r = int(color[0])\n",
    "    g = int(color[1])\n",
    "    b = int(color[2])\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontScale = 0.5\n",
    "    fontThickness = 1\n",
    "\n",
    "    x1, y1, x2, y2 = bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "    x1 = int(x1)\n",
    "    y1 = int(y1)\n",
    "    x2 = int(x2)\n",
    "    y2 = int(y2)\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (b, g, r), thickness)\n",
    "    # Add label\n",
    "    if label is not None:\n",
    "        # Get size of the text box\n",
    "        (tw, th) = cv2.getTextSize(label, font, fontScale, fontThickness)[0]\n",
    "        # Top-left coord of the textbox\n",
    "        (xin_bb, yin_bb) = (x1 + thickness, y1 - th + int(12.5 * fontScale))\n",
    "        # Checking position of the text top-left (outside or inside the bb)\n",
    "        if yin_bb - th <= 0:  # if outside the image\n",
    "            yin_bb = y1 + th  # put it inside the bb\n",
    "        r_Xin = x1 - int(thickness / 2)\n",
    "        r_Yin = y1 - th - int(thickness / 2)\n",
    "        # Draw filled rectangle to put the text in it\n",
    "        cv2.rectangle(image, (r_Xin, r_Yin - thickness),\n",
    "                      (r_Xin + tw + thickness * 3, r_Yin + th + int(12.5 * fontScale)), (b, g, r),\n",
    "                      -1)\n",
    "        cv2.putText(image, label, (xin_bb, yin_bb), font, fontScale, (0, 0, 0), fontThickness,\n",
    "                    cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ae9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BoundingBox:\n",
    "    def __init__(self,\n",
    "                 imageName,\n",
    "                 classId,\n",
    "                 x,\n",
    "                 y,\n",
    "                 w,\n",
    "                 h,\n",
    "                 typeCoordinates=CoordinatesType.Absolute,\n",
    "                 imgSize=None,\n",
    "                 bbType=BBType.GroundTruth,\n",
    "                 classConfidence=None,\n",
    "                 format=BBFormat.XYWH):\n",
    "        self._imageName = imageName\n",
    "        self._typeCoordinates = typeCoordinates\n",
    "        if typeCoordinates == CoordinatesType.Relative and imgSize is None:\n",
    "            raise IOError(\n",
    "                'Parameter \\'imgSize\\' is required. It is necessary to inform the image size.')\n",
    "        if bbType == BBType.Detected and classConfidence is None:\n",
    "            raise IOError(\n",
    "                'For bbType=\\'Detection\\', it is necessary to inform the classConfidence value.')\n",
    "        # if classConfidence != None and (classConfidence < 0 or classConfidence > 1):\n",
    "        # raise IOError('classConfidence value must be a real value between 0 and 1. Value: %f' %\n",
    "        # classConfidence)\n",
    "        self._classConfidence = classConfidence\n",
    "        self._bbType = bbType\n",
    "        self._classId = classId\n",
    "        self._format = format\n",
    "        # If relative coordinates, convert to absolute values\n",
    "        # For relative coords: (x,y,w,h)=(X_center/img_width , Y_center/img_height)\n",
    "        if (typeCoordinates == CoordinatesType.Relative):\n",
    "            (self._x, self._y, self._w, self._h) = convertToAbsoluteValues(imgSize, (x, y, w, h))\n",
    "            self._width_img = imgSize[0]\n",
    "            self._height_img = imgSize[1]\n",
    "            if format == BBFormat.XYWH:\n",
    "                self._x2 = self._w\n",
    "                self._y2 = self._h\n",
    "                self._w = self._x2 - self._x\n",
    "                self._h = self._y2 - self._y\n",
    "            else:\n",
    "                raise IOError(\n",
    "                    'For relative coordinates, the format must be XYWH (x,y,width,height)')\n",
    "        # For absolute coords: (x,y,w,h)=real bb coords\n",
    "        else:\n",
    "            self._x = x\n",
    "            self._y = y\n",
    "            if format == BBFormat.XYWH:\n",
    "                self._w = w\n",
    "                self._h = h\n",
    "                self._x2 = self._x + self._w\n",
    "                self._y2 = self._y + self._h\n",
    "            else:  # format == BBFormat.XYX2Y2: <left> <top> <right> <bottom>.\n",
    "                self._x2 = w\n",
    "                self._y2 = h\n",
    "                self._w = self._x2 - self._x\n",
    "                self._h = self._y2 - self._y\n",
    "        if imgSize is None:\n",
    "            self._width_img = None\n",
    "            self._height_img = None\n",
    "        else:\n",
    "            self._width_img = imgSize[0]\n",
    "            self._height_img = imgSize[1]\n",
    "    def getAbsoluteBoundingBox(self, format=BBFormat.XYWH):\n",
    "        if format == BBFormat.XYWH:\n",
    "            return (self._x, self._y, self._w, self._h)\n",
    "        elif format == BBFormat.XYX2Y2:\n",
    "            return (self._x, self._y, self._x2, self._y2)\n",
    "    def getRelativeBoundingBox(self, imgSize=None):\n",
    "        if imgSize is None and self._width_img is None and self._height_img is None:\n",
    "            raise IOError(\n",
    "                'Parameter \\'imgSize\\' is required. It is necessary to inform the image size.')\n",
    "        if imgSize is None:\n",
    "            return convertToRelativeValues((imgSize[0], imgSize[1]),\n",
    "                                           (self._x, self._y, self._w, self._h))\n",
    "        else:\n",
    "            return convertToRelativeValues((self._width_img, self._height_img),\n",
    "                                           (self._x, self._y, self._w, self._h))\n",
    "    def getImageName(self):\n",
    "        return self._imageName\n",
    "    def getConfidence(self):\n",
    "        return self._classConfidence\n",
    "    def getFormat(self):\n",
    "        return self._format\n",
    "    def getClassId(self):\n",
    "        return self._classId\n",
    "    def getImageSize(self):\n",
    "        return (self._width_img, self._height_img)\n",
    "    def getCoordinatesType(self):\n",
    "        return self._typeCoordinates\n",
    "    def getBBType(self):\n",
    "        return self._bbType\n",
    "    @staticmethod\n",
    "    def compare(det1, det2):\n",
    "        det1BB = det1.getAbsoluteBoundingBox()\n",
    "        det1ImgSize = det1.getImageSize()\n",
    "        det2BB = det2.getAbsoluteBoundingBox()\n",
    "        det2ImgSize = det2.getImageSize()\n",
    "        if det1.getClassId() == det2.getClassId() and \\\n",
    "           det1.classConfidence == det2.classConfidenc() and \\\n",
    "           det1BB[0] == det2BB[0] and \\\n",
    "           det1BB[1] == det2BB[1] and \\\n",
    "           det1BB[2] == det2BB[2] and \\\n",
    "           det1BB[3] == det2BB[3] and \\\n",
    "           det1ImgSize[0] == det1ImgSize[0] and \\\n",
    "           det2ImgSize[1] == det2ImgSize[1]:\n",
    "            return True\n",
    "        return False\n",
    "    @staticmethod\n",
    "    def clone(boundingBox):\n",
    "        absBB = boundingBox.getAbsoluteBoundingBox(format=BBFormat.XYWH)\n",
    "        # return (self._x,self._y,self._x2,self._y2)\n",
    "        newBoundingBox = BoundingBox(\n",
    "            boundingBox.getImageName(),\n",
    "            boundingBox.getClassId(),\n",
    "            absBB[0],\n",
    "            absBB[1],\n",
    "            absBB[2],\n",
    "            absBB[3],\n",
    "            typeCoordinates=boundingBox.getCoordinatesType(),\n",
    "            imgSize=boundingBox.getImageSize(),\n",
    "            bbType=boundingBox.getBBType(),\n",
    "            classConfidence=boundingBox.getConfidence(),\n",
    "            format=BBFormat.XYWH)\n",
    "        return newBoundingBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BoundingBoxes:\n",
    "    def __init__(self):\n",
    "        self._boundingBoxes = []\n",
    "    def addBoundingBox(self, bb):\n",
    "        self._boundingBoxes.append(bb)\n",
    "    def removeBoundingBox(self, _boundingBox):\n",
    "        for d in self._boundingBoxes:\n",
    "            if BoundingBox.compare(d, _boundingBox):\n",
    "                del self._boundingBoxes[d]\n",
    "                return\n",
    "    def removeAllBoundingBoxes(self):\n",
    "        self._boundingBoxes = []\n",
    "    def getBoundingBoxes(self):\n",
    "        return self._boundingBoxes\n",
    "    def getBoundingBoxByClass(self, classId):\n",
    "        boundingBoxes = []\n",
    "        for d in self._boundingBoxes:\n",
    "            if d.getClassId() == classId:\n",
    "                boundingBoxes.append(d)\n",
    "        return boundingBoxes\n",
    "    def getClasses(self):\n",
    "        classes = []\n",
    "        for d in self._boundingBoxes:\n",
    "            c = d.getClassId()\n",
    "            if c not in classes:\n",
    "                classes.append(c)\n",
    "        return classes\n",
    "    def getBoundingBoxesByType(self, bbType):\n",
    "        return [d for d in self._boundingBoxes if d.getBBType() == bbType]\n",
    "    def getBoundingBoxesByImageName(self, imageName):\n",
    "        return [d for d in self._boundingBoxes if d.getImageName() == imageName]\n",
    "    def count(self, bbType=None):\n",
    "        if bbType is None: \n",
    "            return len(self._boundingBoxes)\n",
    "        count = 0\n",
    "        for d in self._boundingBoxes:\n",
    "            if d.getBBType() == bbType:\n",
    "                count += 1\n",
    "        return count\n",
    "    def clone(self):\n",
    "        newBoundingBoxes = BoundingBoxes()\n",
    "        for d in self._boundingBoxes:\n",
    "            det = BoundingBox.clone(d)\n",
    "            newBoundingBoxes.addBoundingBox(det)\n",
    "        return newBoundingBoxes\n",
    "    def drawAllBoundingBoxes(self, image, imageName):\n",
    "        bbxes = self.getBoundingBoxesByImageName(imageName)\n",
    "        for bb in bbxes:\n",
    "            if bb.getBBType() == BBType.GroundTruth:  # if ground truth\n",
    "                image = add_bb_into_image(image, bb, color=(0, 255, 0))  # green\n",
    "            else:  # if detection\n",
    "                image = add_bb_into_image(image, bb, color=(255, 0, 0))  # red\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d191676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tlbr2xyxy(box, img_size=(224,224)):\n",
    "    h,w = img_size\n",
    "    box = box.squeeze()\n",
    "    box = (box + 1) / 2\n",
    "    x1 = int(box[0]*w)\n",
    "    x2 = int(box[2]*w)\n",
    "    y1 = int(box[1]*h)\n",
    "    y2 = int(box[3]*h)\n",
    "    return [x1,y1,x2,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class mAP(Metric):\n",
    "    def __init__(self):\n",
    "        self.boxes = BoundingBoxes()\n",
    "        self.count = 0\n",
    "        self.res = None \n",
    "    def reset(self):\n",
    "        self.boxes.removeAllBoundingBoxes()\n",
    "        self.count = 0\n",
    "    def accumulate(self, learn):\n",
    "        pred_boxes, pred_scores = learn.pred\n",
    "        pred_cls = pred_scores.argmax(dim=-1)\n",
    "        gt_boxes, gt_cls = learn.yb\n",
    "        for img_box_pred, img_score_pred, img_box_gt, img_cls_gt in zip(pred_boxes, pred_scores, gt_boxes, gt_cls): \n",
    "            \n",
    "            pred_nonzero_idxs = (img_score_pred.sum(dim=-1) < 5).float().nonzero()\n",
    "            if not pred_nonzero_idxs.numel() == 0:\n",
    "                img_cls_pred = img_score_pred[pred_nonzero_idxs].argmax(dim=-1)\n",
    "                for box_pred, cls_pred, score_pred in zip(img_box_pred[pred_nonzero_idxs], img_cls_pred, img_score_pred[pred_nonzero_idxs]):\n",
    "                    b = BoundingBox(self.count, learn.dls.vocab[cls_pred.item()+1], *tlbr2xyxy(box_pred), \n",
    "                                bbType=BBType.Detected, format=BBFormat.XYX2Y2, classConfidence=score_pred.squeeze()[cls_pred.item()])\n",
    "                    self.boxes.addBoundingBox(b)\n",
    "            gt_nonzero_idxs   = img_cls_gt.nonzero()#.squeeze()\n",
    "            for box_gt, cls_gt in zip(img_box_gt[gt_nonzero_idxs], img_cls_gt[gt_nonzero_idxs]):\n",
    "                b = BoundingBox(self.count, learn.dls.vocab[cls_gt.item()], *tlbr2xyxy(box_gt), \n",
    "                            bbType=BBType.GroundTruth, format=BBFormat.XYX2Y2)\n",
    "                self.boxes.addBoundingBox(b)\n",
    "            self.count += 1\n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.boxes.getBoundingBoxes()) == 0:\n",
    "            return 0\n",
    "        self.res = Evaluator().GetPascalVOCMetrics(self.boxes)\n",
    "        return np.mean([cat[\"AP\"] for cat in self.res])\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"mAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb72fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Evaluator:\n",
    "    def GetPascalVOCMetrics(self,\n",
    "                            boundingboxes,\n",
    "                            IOUThreshold=0.5,\n",
    "                            method=MethodAveragePrecision.EveryPointInterpolation):\n",
    "        \"\"\"Get the metrics used by the VOC Pascal 2012 challenge.\n",
    "        Get\n",
    "        Args:\n",
    "            boundingboxes: Object of the class BoundingBoxes representing ground truth and detected\n",
    "            bounding boxes;\n",
    "            IOUThreshold: IOU threshold indicating which detections will be considered TP or FP\n",
    "            (default value = 0.5);\n",
    "            method (default = EveryPointInterpolation): It can be calculated as the implementation\n",
    "            in the official PASCAL VOC toolkit (EveryPointInterpolation), or applying the 11-point\n",
    "            interpolatio as described in the paper \"The PASCAL Visual Object Classes(VOC) Challenge\"\n",
    "            or EveryPointInterpolation\"  (ElevenPointInterpolation);\n",
    "        Returns:\n",
    "            A list of dictionaries. Each dictionary contains information and metrics of each class.\n",
    "            The keys of each dictionary are:\n",
    "            dict['class']: class representing the current dictionary;\n",
    "            dict['precision']: array with the precision values;\n",
    "            dict['recall']: array with the recall values;\n",
    "            dict['AP']: average precision;\n",
    "            dict['interpolated precision']: interpolated precision values;\n",
    "            dict['interpolated recall']: interpolated recall values;\n",
    "            dict['total positives']: total number of ground truth positives;\n",
    "            dict['total TP']: total number of True Positive detections;\n",
    "            dict['total FP']: total number of False Negative detections;\n",
    "        \"\"\"\n",
    "        ret = []  # list containing metrics (precision, recall, average precision) of each class\n",
    "        # List with all ground truths (Ex: [imageName,class,confidence=1, (bb coordinates XYX2Y2)])\n",
    "        groundTruths = []\n",
    "        # List with all detections (Ex: [imageName,class,confidence,(bb coordinates XYX2Y2)])\n",
    "        detections = []\n",
    "        # Get all classes\n",
    "        classes = []\n",
    "        # Loop through all bounding boxes and separate them into GTs and detections\n",
    "        for bb in boundingboxes.getBoundingBoxes():\n",
    "            # [imageName, class, confidence, (bb coordinates XYX2Y2)]\n",
    "            if bb.getBBType() == BBType.GroundTruth:\n",
    "                groundTruths.append([\n",
    "                    bb.getImageName(),\n",
    "                    bb.getClassId(), 1,\n",
    "                    bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "                ])\n",
    "            else:\n",
    "                detections.append([\n",
    "                    bb.getImageName(),\n",
    "                    bb.getClassId(),\n",
    "                    bb.getConfidence(),\n",
    "                    bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "                ])\n",
    "            # get class\n",
    "            if bb.getClassId() not in classes:\n",
    "                classes.append(bb.getClassId())\n",
    "        classes = sorted(classes)\n",
    "        # Precision x Recall is obtained individually by each class\n",
    "        # Loop through by classes\n",
    "        for c in classes:\n",
    "            # Get only detection of class c\n",
    "            dects = []\n",
    "            [dects.append(d) for d in detections if d[1] == c]\n",
    "            # Get only ground truths of class c\n",
    "            gts = []\n",
    "            [gts.append(g) for g in groundTruths if g[1] == c]\n",
    "            npos = len(gts)\n",
    "            # sort detections by decreasing confidence\n",
    "            dects = sorted(dects, key=lambda conf: conf[2], reverse=True)\n",
    "            TP = np.zeros(len(dects))\n",
    "            FP = np.zeros(len(dects))\n",
    "            # create dictionary with amount of gts for each image\n",
    "            det = Counter([cc[0] for cc in gts])\n",
    "            for key, val in det.items():\n",
    "                det[key] = np.zeros(val)\n",
    "            # Loop through detections\n",
    "            for d in range(len(dects)):\n",
    "                # Find ground truth image\n",
    "                gt = [gt for gt in gts if gt[0] == dects[d][0]]\n",
    "                iouMax = sys.float_info.min\n",
    "                for j in range(len(gt)):\n",
    "                    # print('Ground truth gt => %s' % (gt[j][3],))\n",
    "                    iou = Evaluator.iou(dects[d][3], gt[j][3])\n",
    "                    if iou > iouMax:\n",
    "                        iouMax = iou\n",
    "                        jmax = j\n",
    "                # Assign detection as true positive/don't care/false positive\n",
    "                if iouMax >= IOUThreshold:\n",
    "                    if det[dects[d][0]][jmax] == 0:\n",
    "                        TP[d] = 1  # count as true positive\n",
    "                        det[dects[d][0]][jmax] = 1  # flag as already 'seen'\n",
    "                        # print(\"TP\")\n",
    "                    else:\n",
    "                        FP[d] = 1  # count as false positive\n",
    "                else:\n",
    "                    FP[d] = 1  # count as false positive\n",
    "            # compute precision, recall and average precision\n",
    "            acc_FP = np.cumsum(FP)\n",
    "            acc_TP = np.cumsum(TP)\n",
    "            rec = acc_TP / npos\n",
    "            prec = np.divide(acc_TP, (acc_FP + acc_TP))\n",
    "            # Depending on the method, call the right implementation\n",
    "            if method == MethodAveragePrecision.EveryPointInterpolation:\n",
    "                [ap, mpre, mrec, ii] = Evaluator.CalculateAveragePrecision(rec, prec)\n",
    "            else:\n",
    "                [ap, mpre, mrec, _] = Evaluator.ElevenPointInterpolatedAP(rec, prec)\n",
    "            # add class result in the dictionary to be returned\n",
    "            r = {\n",
    "                'class': c,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'AP': ap,\n",
    "                'interpolated precision': mpre,\n",
    "                'interpolated recall': mrec,\n",
    "                'total positives': npos,\n",
    "                'total TP': np.sum(TP),\n",
    "                'total FP': np.sum(FP)\n",
    "            }\n",
    "            ret.append(r)\n",
    "        return ret\n",
    "\n",
    "    def PlotPrecisionRecallCurve(self,\n",
    "                                 boundingBoxes,\n",
    "                                 IOUThreshold=0.5,\n",
    "                                 method=MethodAveragePrecision.EveryPointInterpolation,\n",
    "                                 showAP=False,\n",
    "                                 showInterpolatedPrecision=False,\n",
    "                                 savePath=None,\n",
    "                                 showGraphic=True):\n",
    "        \"\"\"PlotPrecisionRecallCurve\n",
    "        Plot the Precision x Recall curve for a given class.\n",
    "        Args:\n",
    "            boundingBoxes: Object of the class BoundingBoxes representing ground truth and detected\n",
    "            bounding boxes;\n",
    "            IOUThreshold (optional): IOU threshold indicating which detections will be considered\n",
    "            TP or FP (default value = 0.5);\n",
    "            method (default = EveryPointInterpolation): It can be calculated as the implementation\n",
    "            in the official PASCAL VOC toolkit (EveryPointInterpolation), or applying the 11-point\n",
    "            interpolatio as described in the paper \"The PASCAL Visual Object Classes(VOC) Challenge\"\n",
    "            or EveryPointInterpolation\"  (ElevenPointInterpolation).\n",
    "            showAP (optional): if True, the average precision value will be shown in the title of\n",
    "            the graph (default = False);\n",
    "            showInterpolatedPrecision (optional): if True, it will show in the plot the interpolated\n",
    "             precision (default = False);\n",
    "            savePath (optional): if informed, the plot will be saved as an image in this path\n",
    "            (ex: /home/mywork/ap.png) (default = None);\n",
    "            showGraphic (optional): if True, the plot will be shown (default = True)\n",
    "        Returns:\n",
    "            A list of dictionaries. Each dictionary contains information and metrics of each class.\n",
    "            The keys of each dictionary are:\n",
    "            dict['class']: class representing the current dictionary;\n",
    "            dict['precision']: array with the precision values;\n",
    "            dict['recall']: array with the recall values;\n",
    "            dict['AP']: average precision;\n",
    "            dict['interpolated precision']: interpolated precision values;\n",
    "            dict['interpolated recall']: interpolated recall values;\n",
    "            dict['total positives']: total number of ground truth positives;\n",
    "            dict['total TP']: total number of True Positive detections;\n",
    "            dict['total FP']: total number of False Negative detections;\n",
    "        \"\"\"\n",
    "        results = self.GetPascalVOCMetrics(boundingBoxes, IOUThreshold, method)\n",
    "        result = None\n",
    "        # Each resut represents a class\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                raise IOError('Error: Class %d could not be found.' % classId)\n",
    "\n",
    "            classId = result['class']\n",
    "            precision = result['precision']\n",
    "            recall = result['recall']\n",
    "            average_precision = result['AP']\n",
    "            mpre = result['interpolated precision']\n",
    "            mrec = result['interpolated recall']\n",
    "            npos = result['total positives']\n",
    "            total_tp = result['total TP']\n",
    "            total_fp = result['total FP']\n",
    "\n",
    "            plt.close()\n",
    "            if showInterpolatedPrecision:\n",
    "                if method == MethodAveragePrecision.EveryPointInterpolation:\n",
    "                    plt.plot(mrec, mpre, '--r', label='Interpolated precision (every point)')\n",
    "                elif method == MethodAveragePrecision.ElevenPointInterpolation:\n",
    "                    # Uncomment the line below if you want to plot the area\n",
    "                    # plt.plot(mrec, mpre, 'or', label='11-point interpolated precision')\n",
    "                    # Remove duplicates, getting only the highest precision of each recall value\n",
    "                    nrec = []\n",
    "                    nprec = []\n",
    "                    for idx in range(len(mrec)):\n",
    "                        r = mrec[idx]\n",
    "                        if r not in nrec:\n",
    "                            idxEq = np.argwhere(mrec == r)\n",
    "                            nrec.append(r)\n",
    "                            nprec.append(max([mpre[int(id)] for id in idxEq]))\n",
    "                    plt.plot(nrec, nprec, 'or', label='11-point interpolated precision')\n",
    "            plt.plot(recall, precision, label='Precision')\n",
    "            plt.xlabel('recall')\n",
    "            plt.ylabel('precision')\n",
    "            if showAP:\n",
    "                ap_str = \"{0:.2f}%\".format(average_precision * 100)\n",
    "                plt.title('Precision x Recall curve \\nClass: %s, AP: %s' % (str(classId), ap_str))\n",
    "            else:\n",
    "                plt.title('Precision x Recall curve \\nClass: %s' % str(classId))\n",
    "            plt.legend(shadow=True)\n",
    "            plt.grid()\n",
    "         \n",
    "            if savePath is not None:\n",
    "                plt.savefig(os.path.join(savePath, classId + '.png'))\n",
    "            if showGraphic is True:\n",
    "                plt.show()\n",
    "                plt.pause(0.05)\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def CalculateAveragePrecision(rec, prec):\n",
    "        mrec = []\n",
    "        mrec.append(0)\n",
    "        [mrec.append(e) for e in rec]\n",
    "        mrec.append(1)\n",
    "        mpre = []\n",
    "        mpre.append(0)\n",
    "        [mpre.append(e) for e in prec]\n",
    "        mpre.append(0)\n",
    "        for i in range(len(mpre) - 1, 0, -1):\n",
    "            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
    "        ii = []\n",
    "        for i in range(len(mrec) - 1):\n",
    "            if mrec[1:][i] != mrec[0:-1][i]:\n",
    "                ii.append(i + 1)\n",
    "        ap = 0\n",
    "        for i in ii:\n",
    "            ap = ap + np.sum((mrec[i] - mrec[i - 1]) * mpre[i])\n",
    "        # return [ap, mpre[1:len(mpre)-1], mrec[1:len(mpre)-1], ii]\n",
    "        return [ap, mpre[0:len(mpre) - 1], mrec[0:len(mpre) - 1], ii]\n",
    "\n",
    "    @staticmethod\n",
    "    # 11-point interpolated average precision\n",
    "    def ElevenPointInterpolatedAP(rec, prec):\n",
    "        # def CalculateAveragePrecision2(rec, prec):\n",
    "        mrec = []\n",
    "        # mrec.append(0)\n",
    "        [mrec.append(e) for e in rec]\n",
    "        # mrec.append(1)\n",
    "        mpre = []\n",
    "        # mpre.append(0)\n",
    "        [mpre.append(e) for e in prec]\n",
    "        # mpre.append(0)\n",
    "        recallValues = np.linspace(0, 1, 11)\n",
    "        recallValues = list(recallValues[::-1])\n",
    "        rhoInterp = []\n",
    "        recallValid = []\n",
    "        # For each recallValues (0, 0.1, 0.2, ... , 1)\n",
    "        for r in recallValues:\n",
    "            # Obtain all recall values higher or equal than r\n",
    "            argGreaterRecalls = np.argwhere(mrec[:] >= r)\n",
    "            pmax = 0\n",
    "            # If there are recalls above r\n",
    "            if argGreaterRecalls.size != 0:\n",
    "                pmax = max(mpre[argGreaterRecalls.min():])\n",
    "            recallValid.append(r)\n",
    "            rhoInterp.append(pmax)\n",
    "        # By definition AP = sum(max(precision whose recall is above r))/11\n",
    "        ap = sum(rhoInterp) / 11\n",
    "        # Generating values for the plot\n",
    "        rvals = []\n",
    "        rvals.append(recallValid[0])\n",
    "        [rvals.append(e) for e in recallValid]\n",
    "        rvals.append(0)\n",
    "        pvals = []\n",
    "        pvals.append(0)\n",
    "        [pvals.append(e) for e in rhoInterp]\n",
    "        pvals.append(0)\n",
    "        # rhoInterp = rhoInterp[::-1]\n",
    "        cc = []\n",
    "        for i in range(len(rvals)):\n",
    "            p = (rvals[i], pvals[i - 1])\n",
    "            if p not in cc:\n",
    "                cc.append(p)\n",
    "            p = (rvals[i], pvals[i])\n",
    "            if p not in cc:\n",
    "                cc.append(p)\n",
    "        recallValues = [i[0] for i in cc]\n",
    "        rhoInterp = [i[1] for i in cc]\n",
    "        return [ap, rhoInterp, recallValues, None]\n",
    "\n",
    "    # For each detections, calculate IOU with reference\n",
    "    @staticmethod\n",
    "    def _getAllIOUs(reference, detections):\n",
    "        ret = []\n",
    "        bbReference = reference.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "        # img = np.zeros((200,200,3), np.uint8)\n",
    "        for d in detections:\n",
    "            bb = d.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "            iou = Evaluator.iou(bbReference, bb)\n",
    "            # Show blank image with the bounding boxes\n",
    "            # img = add_bb_into_image(img, d, color=(255,0,0), thickness=2, label=None)\n",
    "            # img = add_bb_into_image(img, reference, color=(0,255,0), thickness=2, label=None)\n",
    "            ret.append((iou, reference, d))  # iou, reference, detection\n",
    "        return sorted(ret, key=lambda i: i[0], reverse=True)  # sort by iou (from highest to lowest)\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(boxA, boxB):\n",
    "        # if boxes dont intersect\n",
    "        if Evaluator._boxesIntersect(boxA, boxB) is False:\n",
    "            return 0\n",
    "        interArea = Evaluator._getIntersectionArea(boxA, boxB)\n",
    "        union = Evaluator._getUnionAreas(boxA, boxB, interArea=interArea)\n",
    "        # intersection over union\n",
    "        print(interArea)\n",
    "        print(union)\n",
    "        if interArea == 0:\n",
    "            interArea == 0.1\n",
    "        if union == 0:\n",
    "            union == 0.1   \n",
    "        iou = interArea / union\n",
    "        #assert iou >= 0\n",
    "        return iou\n",
    "\n",
    "    # boxA = (Ax1,Ay1,Ax2,Ay2)\n",
    "    # boxB = (Bx1,By1,Bx2,By2)\n",
    "    @staticmethod\n",
    "    def _boxesIntersect(boxA, boxB):\n",
    "        if boxA[0] > boxB[2]:\n",
    "            return False  # boxA is right of boxB\n",
    "        if boxB[0] > boxA[2]:\n",
    "            return False  # boxA is left of boxB\n",
    "        if boxA[3] < boxB[1]:\n",
    "            return False  # boxA is above boxB\n",
    "        if boxA[1] > boxB[3]:\n",
    "            return False  # boxA is below boxB\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _getIntersectionArea(boxA, boxB):\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "        # intersection area\n",
    "        return (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _getUnionAreas(boxA, boxB, interArea=None):\n",
    "        area_A = Evaluator._getArea(boxA)\n",
    "        area_B = Evaluator._getArea(boxB)\n",
    "        if interArea is None:\n",
    "            interArea = Evaluator._getIntersectionArea(boxA, boxB)\n",
    "        return float(area_A + area_B - interArea)\n",
    "\n",
    "    @staticmethod\n",
    "    def _getArea(box):\n",
    "        return (box[2] - box[0] + 1) * (box[3] - box[1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ec1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_explore.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 04_pipeline.ipynb.\n",
      "Converted 05_train.ipynb.\n",
      "Converted 06_examine.ipynb.\n",
      "Converted 10_wearable.ipynb.\n",
      "Converted 20_retinanet.ipynb.\n",
      "Converted 90_tutorial.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6f350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
